{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "## Training\n",
    "Since CondBERT uses pre-trained models, such as [BERT](https://huggingface.co/docs/transformers/model_doc/bert) (for masking), [FastText](https://pypi.org/project/fasttext/) (for words similarity) models and [sentence transformers](https://pypi.org/project/sentence-transformers/) models (for sentence similarity), no additional training is required.\n",
    "\n",
    "However one may want to build vocab based on custom dataset, which could be down by running (where custom dataset should be located in *[data/interim](../data/interim)* in the same format as described in [previous notebook](1.0-initial-data-exploration.ipynb)):\n",
    "\n",
    "Python\r\n",
    "```python\r\n",
    "from src.data.build_vocab import build_vocab\r\n",
    "build_vocab()`\n",
    "```\r\n",
    "\r\n",
    "CLI\r\n",
    "```shell\r\n",
    "cd src/data\r\n",
    "python make_data`et.py\r\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocab with custom Masked Language Model\n",
    "It's also possible to build vocabulary using different model, for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 135390/135390 [00:43<00:00, 3122.01it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 135390/135390 [00:42<00:00, 3192.81it/s]\n"
     ]
    }
   ],
   "source": [
    "from src.data.build_vocab import build_vocab\n",
    "\n",
    "# here model_name is model from https://huggingface.co/models?pipeline_tag=fill-mask\n",
    "model_name = 'bert-large-uncased'\n",
    "build_vocab(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference with custom parameters\n",
    "It's also possible to run inference with custom parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am not crazy!\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "import fasttext.util\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "from pathlib import Path\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "from src.models.cond_BERT import CondBERT\n",
    "from src.data.load_vocab import load_toxicities, load_word2coef\n",
    "\n",
    "vocab_dirname = Path('../data/interim/vocab')\n",
    "# here, instead of:\n",
    "# condBERT = load_condBERT()\n",
    "# do:\n",
    "model = BertForMaskedLM.from_pretrained(model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "# array of token toxicities\n",
    "tok_toxicities = load_toxicities(vocab_dirname / 'token_toxicities.txt')\n",
    "# word-to-coefficient mapping\n",
    "word2coef = load_word2coef(vocab_dirname / 'word2coef.pkl')\n",
    "# here you can load custom fasttext model to find words similarity\n",
    "fasttext.util.download_model('en', if_exists='ignore')\n",
    "ft = fasttext.load_model('cc.en.300.bin')\n",
    "condBERT = CondBERT(model, tokenizer, device, tok_toxicities, word2coef, ft)\n",
    "\n",
    "toxic_example = \"I am not stupid!\"\n",
    "print(condBERT(toxic_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "Speaking about visualization, I believe that information provided in **docs, reports, README and notebooks** is sufficient and no additional visualization is required.\n",
    "\n",
    "![Important graph](figures/main.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
